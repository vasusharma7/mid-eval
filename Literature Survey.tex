%_____________________________________________________________________________________________ 
% LATEX Template: Department of Comp/IT BTech Project Reports
% Sample Chapter
% Sun Mar 27 10:25:35 IST 2011
%
% Note: Itemization, enumeration and other things not shown. A sample figure is included.
%_____________________________________________________________________________________________ 
{\let\clearpage\relax \chapter{Literature Survey}}

\textbf{Efficient, Lexicon-Free OCR using Deep Learning [2019]\cite{1}:}
This paper tries to address the general OCR problem for both printed and scene text using segmentation free recognition methods.The approach suggested by authors uses Convolutional Neural Networks to extract latent representations of input images,thus increasing robustness to local distortions.Then the features extracted by
CNNs were combined and fed into the LSTM network with a Connectionist Temporal Classification (CTC) Loss function.The authors also propose a novel data augmentation technique that improves the robustness of neural models for text recognition and the model is trained purely on syntheticcally generated documents.

\textbf{U-Net: Convolutional Networks for Biomedical Image Segmentation [2015]\cite{2}:}
This paper addresses the per-pixel segmentation problem. The network architecture consists of several downsampling layers which aim to extract the low level features from the image and the output after this stage is an image with very high number of channels and very small height and width dimensions. This output is then passed to upsampling layers which consist of transpose convolutional operations as opposed to usual max pooling operation.There are skip connections from downsampling layers to upsampling layers as the later layers forget the spatial information in the input image which is required to do segmentation.This upsampling is followed by fully connected layers which producce the output of same dimension as input in height and width and number of layers being equal to the number of segmentation classes.

\textbf{ARU-Net: A Two-Stage Method for Text Line Detection in Historical Documents [2019]\cite{3}:}
This paper builds on top of U-Net Architecture. The work done in the paper focuses on identifying the text lines on the documents.The authors have proposed an approach with some additions to U-Net to draw the baselines for the text lines and also to draw the line starting and ending demarcations as it can be crucial for multi columnar documents. The appoach is to combine Residual Network\cite{4} and attention to the layers used in U-Net. As suggested by the authors, there is loss of spatial information as we go deep inside the convolutional layers in the U-net, so the ResNets improve the representation power.Furthermore, spatial attention mechanism is developed which allows the ARU-Net to focus on image content at different positions and scales. The authors then use the baselines predicted using deep learning approach to further reach at text level segmentation using some heuristics


\textbf{Character Region Awareness for Text Detection [2019]\cite{5}:}
This paper as opposed to other scene based text detection methods based on neural networks which use explict bounding boxes for making predictions, uses character level information and affinity betweeen different characters.The network is inspired by U-Net based upsampling architecture succeeded by VGG-Net for downsampling.The output of the model are two images - one consisting of character region scores and the other consisting of affinity scores between the characters. Since, it can be a difficult to obtain character level bounding box information for ground truth data, the authors have suggested using 2 slightly different architectures - one of them is fully supervised learning based which trained on synthetic character level data generated by the researchers while the other is semi-supervised approach where the character level ground truth is generated using the interim model learned by training on the synthetic data. The final inference for word level bounding boxes is generated using some open-cv techniques wherein connected component analysis is done on the region and affinity scores together which brings out the bounding boxes for the words.The network produces really good results but at the same time is very computationally expensive to train, especially in the semi supervises phase where pseudo ground truths are generated during the training.

\textbf{READ-BAD: A New Dataset and Evaluation Scheme for Baseline Detection in Archival Documents [2017]\cite{6}:}
A baseline is defined in the typographical sense as the virtual line where most characters rest upon and descenders extend below. About 2000 document images from each of 9 different European archives were collected. These documents were written between 1470 and 1930. The authors sampled 250 images from each archival collection using freely available python scripts.This results in a set of 2500 document images. After removing images due to quality as well as content issues the number reduced from 2250 to 2118. For these images the text regions as well as baselines were annotated by DigiTexx. The well-known PAGE XML3 scheme is used for storing text region and baseline information


%_____________________________________________________________________________________________ 
